[[34m2023-11-14 16:33:22,661[0m] {[34mscheduler_job.py:[0m714} INFO[0m - Starting the scheduler[0m
[[34m2023-11-14 16:33:22,662[0m] {[34mscheduler_job.py:[0m719} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-11-14 16:33:22,664[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-11-14 16:33:22,671[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 18130[0m
[[34m2023-11-14 16:33:22,673[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-11-14 16:33:22,676[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-11-14T16:33:22.688+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-11-14 16:34:57,599[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:34:56.152706+00:00 [scheduled]>[0m
[[34m2023-11-14 16:34:57,600[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:34:57,600[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:34:56.152706+00:00 [scheduled]>[0m
[[34m2023-11-14 16:34:57,605[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-11-14T16:34:56.152706+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-11-14 16:34:57,605[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:34:56.152706+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:34:57,648[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:34:56.152706+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:34:58,740[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:34:59,600[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:34:56.152706+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:35:02,239[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-11-14T16:34:56.152706+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:35:02,245[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-11-14T16:34:56.152706+00:00, map_index=-1, run_start_date=2023-11-14 16:34:59.676904+00:00, run_end_date=2023-11-14 16:35:01.798784+00:00, run_duration=2.12188, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-11-14 16:34:57.601437+00:00, queued_by_job_id=2, pid=18748[0m
[[34m2023-11-14 16:35:03,317[0m] {[34mdagrun.py:[0m586} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2023-11-14 16:34:56.152706+00:00: manual__2023-11-14T16:34:56.152706+00:00, state:running, queued_at: 2023-11-14 16:34:56.209796+00:00. externally triggered: True> failed[0m
[[34m2023-11-14 16:35:03,317[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-11-14 16:34:56.152706+00:00, run_id=manual__2023-11-14T16:34:56.152706+00:00, run_start_date=2023-11-14 16:34:57.137840+00:00, run_end_date=2023-11-14 16:35:03.317603+00:00, run_duration=6.179763, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-11-14 16:34:56.152706+00:00, data_interval_end=2023-11-14 16:34:56.152706+00:00, dag_hash=5818f141addb9d3831191a31d6d75df9[0m
[[34m2023-11-14 16:35:03,320[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-11-14 16:36:13,823[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:13,823[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:36:13,823[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:13,824[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-11-14T16:36:12.485098+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-11-14 16:36:13,825[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:13,868[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:15,024[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:36:15,858[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:12.485098+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:36:18,348[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-11-14T16:36:12.485098+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:36:18,351[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-11-14T16:36:12.485098+00:00, map_index=-1, run_start_date=2023-11-14 16:36:15.943969+00:00, run_end_date=2023-11-14 16:36:17.857809+00:00, run_duration=1.91384, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-11-14 16:36:13.823835+00:00, queued_by_job_id=2, pid=19247[0m
[[34m2023-11-14 16:36:18,501[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:18,501[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:36:18,501[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:18,503[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-11-14T16:36:12.485098+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-11-14 16:36:18,503[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:18,544[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:19,591[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:36:20,308[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:12.485098+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:36:21,224[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-11-14T16:36:12.485098+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:36:21,228[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-11-14T16:36:12.485098+00:00, map_index=-1, run_start_date=2023-11-14 16:36:20.387395+00:00, run_end_date=2023-11-14 16:36:20.752509+00:00, run_duration=0.365114, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-11-14 16:36:18.502041+00:00, queued_by_job_id=2, pid=19284[0m
[[34m2023-11-14 16:36:21,305[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:21,306[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:36:21,306[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:12.485098+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:21,310[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-11-14T16:36:12.485098+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-11-14 16:36:21,311[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:21,353[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-11-14T16:36:12.485098+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:22,245[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:36:22,950[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:12.485098+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:36:23,935[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-11-14T16:36:12.485098+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:36:23,938[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-11-14T16:36:12.485098+00:00, map_index=-1, run_start_date=2023-11-14 16:36:23.018511+00:00, run_end_date=2023-11-14 16:36:23.486268+00:00, run_duration=0.467757, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-11-14 16:36:21.307715+00:00, queued_by_job_id=2, pid=19299[0m
[[34m2023-11-14 16:36:24,014[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-11-14 16:36:12.485098+00:00: manual__2023-11-14T16:36:12.485098+00:00, state:running, queued_at: 2023-11-14 16:36:12.489243+00:00. externally triggered: True> successful[0m
[[34m2023-11-14 16:36:24,014[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-11-14 16:36:12.485098+00:00, run_id=manual__2023-11-14T16:36:12.485098+00:00, run_start_date=2023-11-14 16:36:13.716854+00:00, run_end_date=2023-11-14 16:36:24.014478+00:00, run_duration=10.297624, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-11-14 16:36:12.485098+00:00, data_interval_end=2023-11-14 16:36:12.485098+00:00, dag_hash=928c1a40cd8162f40a674cbb36a96ae3[0m
[[34m2023-11-14 16:36:24,016[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-11-14 16:36:54,136[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:54,137[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:36:54,137[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:54,138[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-11-14T16:36:53.751126+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-11-14 16:36:54,138[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:54,179[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:55,359[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:36:56,283[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-11-14T16:36:53.751126+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:36:59,101[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-11-14T16:36:53.751126+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:36:59,106[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-11-14T16:36:53.751126+00:00, map_index=-1, run_start_date=2023-11-14 16:36:56.751738+00:00, run_end_date=2023-11-14 16:36:58.659858+00:00, run_duration=1.90812, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-11-14 16:36:54.137693+00:00, queued_by_job_id=2, pid=19499[0m
[[34m2023-11-14 16:36:59,236[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:59,236[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:36:59,236[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:36:59,238[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-11-14T16:36:53.751126+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-11-14 16:36:59,238[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:36:59,284[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:37:00,383[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:37:01,096[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-11-14T16:36:53.751126+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:37:02,015[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-11-14T16:36:53.751126+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:37:02,018[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-11-14T16:36:53.751126+00:00, map_index=-1, run_start_date=2023-11-14 16:37:01.171665+00:00, run_end_date=2023-11-14 16:37:01.546119+00:00, run_duration=0.374454, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-11-14 16:36:59.237038+00:00, queued_by_job_id=2, pid=19539[0m
[[34m2023-11-14 16:37:02,089[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:37:02,089[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-11-14 16:37:02,090[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:53.751126+00:00 [scheduled]>[0m
[[34m2023-11-14 16:37:02,092[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-11-14T16:36:53.751126+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-11-14 16:37:02,093[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:37:02,135[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-11-14T16:36:53.751126+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2023-11-14 16:37:03,067[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2023-11-14 16:37:03,854[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-11-14T16:36:53.751126+00:00 [queued]> on host codespaces-1a8ad4[0m
[[34m2023-11-14 16:37:04,777[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-11-14T16:36:53.751126+00:00 exited with status success for try_number 1[0m
[[34m2023-11-14 16:37:04,780[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-11-14T16:36:53.751126+00:00, map_index=-1, run_start_date=2023-11-14 16:37:03.925927+00:00, run_end_date=2023-11-14 16:37:04.384743+00:00, run_duration=0.458816, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-11-14 16:37:02.090583+00:00, queued_by_job_id=2, pid=19554[0m
[[34m2023-11-14 16:37:05,361[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-11-14 16:36:53.751126+00:00: manual__2023-11-14T16:36:53.751126+00:00, state:running, queued_at: 2023-11-14 16:36:53.760975+00:00. externally triggered: True> successful[0m
[[34m2023-11-14 16:37:05,362[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-11-14 16:36:53.751126+00:00, run_id=manual__2023-11-14T16:36:53.751126+00:00, run_start_date=2023-11-14 16:36:54.040036+00:00, run_end_date=2023-11-14 16:37:05.362016+00:00, run_duration=11.32198, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-11-14 16:36:53.751126+00:00, data_interval_end=2023-11-14 16:36:53.751126+00:00, dag_hash=928c1a40cd8162f40a674cbb36a96ae3[0m
[[34m2023-11-14 16:37:05,363[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-11-14 16:38:22,750[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-11-14 16:43:22,789[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-11-14 16:47:34,100[0m] {[34mscheduler_job.py:[0m179} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2023-11-14 16:47:35,103[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 18130. PIDs of all processes in the group: [18130][0m
[[34m2023-11-14 16:47:35,104[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 18130[0m
[[34m2023-11-14 16:47:35,328[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=18130, status='terminated', exitcode=0, started='16:33:22') (18130) terminated with exit code 0[0m
[[34m2023-11-14 16:47:35,330[0m] {[34mprocess_utils.py:[0m129} INFO[0m - Sending Signals.SIGTERM to group 18130. PIDs of all processes in the group: [][0m
[[34m2023-11-14 16:47:35,330[0m] {[34mprocess_utils.py:[0m84} INFO[0m - Sending the signal Signals.SIGTERM to group 18130[0m
[[34m2023-11-14 16:47:35,331[0m] {[34mprocess_utils.py:[0m98} INFO[0m - Sending the signal Signals.SIGTERM to process 18130 as process group is missing.[0m
[[34m2023-11-14 16:47:35,331[0m] {[34mscheduler_job.py:[0m788} INFO[0m - Exited execute loop[0m
